CSE 517 Final Project
Qiao Zhang, Haichen Shen, Danyang Zhuo

== Things we tried but failed to get working:
1) Implement character level LSTM using TensorFlow and Mxnet
Our initial goal is to get LSTM working and beat ngrams. We tried two packages.
https://github.com/sherjilozair/char-rnn-tensorflow is a ready made LSTM
implementation in TensorFlow that we started with. We also tried modifying
https://github.com/dmlc/mxnet/blob/master/example/rnn/char_lstm.ipynb
for Mxnet. The main difficulty for us was to actually understand how the model
really works. We understand that LSTM has a stacked layer of LSTM cells that
once we feed sequence of characters in, the last layer would give us a
probability vector that represents the distribution in each possible character.
Since UTF-8 has 120,737 symbols, we didn't know if it was possible to have
that many output classes and still be able to train the model effectively.
We thought about hierarchical classification at the last layer, but we didn't
know enough to implement it. Any difficuly is that LSTM is often trained in
mini-batches. The input sequences are divided into batches. While we can train
in mini-batches, the testing has to be one character at a time. And we weren't
not confident enough in our knowledge of the model to get this modification
correct. We eventually decided that since the final evaluation is through
leaderboard ranking, we should focus on getting a model working and working
well, so we resorted to improving ngrams -- less intellectual value but a
more direct route to improving leaderboard ranking.

2) Kneser-Ney smoothing.
Fill in by Danyang Zhuo.

== Answers to Project Questions
1) How does our language model (LM) work?
Our LM uses interpolated and smoothed trigrams. Specifically, We compute the
conditional probability P(c3|c1c2) by interpolating among unigram, bigram and
trigram, i.e.
P(c3|c1c2) = l1*P(c3) + l2*P(c3|c2) + (1-l1-l2)*P(c3|c1c2)

However, for an unseen character c3 in test set, We can still get zero value for
P(c3|c1c2), so we use additive-k smoothing to handle gaps in ngrams, i.e.

P(c3) = c(c3) + k1 / c() + k1*V
P(c3|c2) = c(c2c3) + k2 / c(c2) + k2*V
P(c3|c1c2) = c(c1c2c3) + k3 / c(c1c2) + k3*V

where V is the vocabulary size, 65392 in our case.

We use the A1 test data as our holdout data to tune l1,l2 and k1,k2,k3.
This A1 test data was never part of the training data, so we felt safe in
using it for holdout. We also implemented a simple grid search to sweep the
parameter space.

Now, We can use the chain rule to compute P(c1c2...cN) for any sentence.

2) What dataset did we use?

We used Wikipedia dump for the month of January, for all available languages.
http://dumps.wikimedia.org/backup-index-bydb.html

We also obtained data for Twitter, Blogs, Newswire.
Fill in by Haichen.

In each dataset, we also artifically add two start symbols to the beginning of
text and one end of text symbol at the end.

Finally, our LM, on startup, would load up the python dictionary blob, add two
start symbols to history. With ngram counts and the various parameters, our LM 
is ready for use!

3) What tools did we use?
Self-designed scrapers for NYTimes.
GNU Parallel as the poorman's MapReduce.

4) Any help we got?
None.

== Answer to Exercise.
Q: Show that P(c|natural) is monotonic in p(natural|c), under reasonable
assumptions.
A: Check out cse517_project.pdf.






