CSE 517 Final Project
Qiao Zhang, Haichen Shen, Danyang Zhuo

== Things we tried but failed to get working:
1) Implement character level LSTM using TensorFlow and Mxnet
Our initial goal is to get LSTM working and beat ngrams. We tried two packages.
https://github.com/sherjilozair/char-rnn-tensorflow is a ready made LSTM
implementation in TensorFlow that we started with. We also tried modifying
https://github.com/dmlc/mxnet/blob/master/example/rnn/char_lstm.ipynb
for Mxnet. The main difficulty for us was to actually understand how the model
really works. We understand that LSTM has a stacked layer of LSTM cells that
once we feed sequence of characters in, the last layer would give us a
probability vector that represents the distribution in each possible character.
Since UTF-8 has 120,737 symbols, we didn't know if it was possible to have
that many output classes and still be able to train the model effectively.
We thought about hierarchical classification at the last layer, but we didn't
know enough to implement it. Any difficuly is that LSTM is often trained in
mini-batches. The input sequences are divided into batches. While we can train
in mini-batches, the testing has to be one character at a time. And we weren't
not confident enough in our knowledge of the model to get this modification
correct. We eventually decided that since the final evaluation is through
leaderboard ranking, we should focus on getting a model working and working
well, so we resorted to improving ngrams -- less intellectual value but a
more direct route to improving leaderboard ranking.

2) Implement Kneser-Ney smoothing.
In our submission, we use simple additive-k smoothing to handle characters or histories that are not seen in our training set. In our previous submissions, we realize better smoothing techniques can help us get better results in languages that have large size of vocabulary. Kneser-Ney is the most effective smoothing techniques in ngram models. The intuition is the following: if 'h' is always following 't', we want to lower the probability of 'h' in the unigram models when we are doing interpolation. Formally, we want the conditional probability to be (assuming a is followed by b),
P(b|a) = max(count(ab) - delta)/count(a) + lambda * |{c| count(cb)>0}|/|{d | count(de) > 0}|.
Lambda is properly set so that the total probabilities sum to 1.
This is the model of bigram Kneser-Ney smoothing and the model for trigram is one order of magnitude more complex. The main bottleneck is that it is computationally too intensive to calculate a term like |{c| count(cb)>0}| for every possible b in our model. We use simple python dictionaries to store our bigram and trigram counts. Without proper indexing, it will take O(Vocabulary^2) of time to calculate |{c| count(cb)>0}|. For trigram, it will take O(Vocabulary^3). With limited time to work on this project, we only implemented the bigram version of Kneser-Ney smoothing but we cannot scale it to trigram. Finally, we decided not to pursue this direction.

== Answers to Project Questions
1) How does our language model (LM) work?
Our LM uses interpolated and smoothed trigrams. Specifically, We compute the
conditional probability P(c3|c1c2) by interpolating among unigram, bigram and
trigram, i.e.
P(c3|c1c2) = l1*P(c3) + l2*P(c3|c2) + (1-l1-l2)*P(c3|c1c2)

However, for an unseen character c3 in test set, We can still get zero value for
P(c3|c1c2), so we use additive-k smoothing to handle gaps in ngrams, i.e.

P(c3) = c(c3) + k1 / c() + k1*V
P(c3|c2) = c(c2c3) + k2 / c(c2) + k2*V
P(c3|c1c2) = c(c1c2c3) + k3 / c(c1c2) + k3*V

where V is the vocabulary size, 65392 in our case.

We use the A1 test data as our holdout data to tune l1,l2 and k1,k2,k3.
This A1 test data was never part of the training data, so we felt safe in
using it for holdout. We also implemented a simple grid search to sweep the
parameter space.

Now, We can use the chain rule to compute P(c1c2...cN) for any sentence.

2) What dataset did we use?

We used Wikipedia dump for the month of January, for all available languages.
http://dumps.wikimedia.org/backup-index-bydb.html

We also obtained data for Twitter, Blogs, Newswire.
Fill in by Haichen.

In each dataset, we also artifically add two start symbols to the beginning of
text and one end of text symbol at the end.

Finally, our LM, on startup, would load up the python dictionary blob, add two
start symbols to history. With ngram counts and the various parameters, our LM
is ready for use!

3) What tools did we use?
Self-designed scrapers for NYTimes.
GNU Parallel as the poorman's MapReduce.

4) Any help we got?
None.

== Answer to Exercise.
Q: Show that P(c|natural) is monotonic in p(natural|c), under reasonable
assumptions.
A: Check out cse517_project.pdf.
